{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two changes to be made in PRML library before you begin:\n",
    " * Go to prml --> linear --> logistic_regression.py and change astype(np.int) to astype(int) under `classify` function.\n",
    " * Go to prml --> linear --> fishers_linear_discriminant.py and change astype(np.int) to astype(int) under `classify` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from prml.preprocess import PolynomialFeature\n",
    "from prml.linear import (\n",
    "    BayesianLogisticRegression,\n",
    "    LeastSquaresClassifier,\n",
    "    FishersLinearDiscriminant,\n",
    "    LogisticRegression,\n",
    "    Perceptron,\n",
    "    SoftmaxRegression\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(add_outliers=False, add_class=False):\n",
    "    x0 = np.random.normal(size=1000).reshape(-1, 2) - 1.5\n",
    "    x1 = np.random.normal(size=1000).reshape(-1, 2) + 1.5\n",
    "    if add_outliers:\n",
    "        x_1 = np.random.normal(size=200).reshape(-1, 2) + np.array([6.5, 9.5])\n",
    "        return np.concatenate([x0, x1, x_1]), np.concatenate([np.zeros(700), np.ones(400)]).astype(int)\n",
    "    if add_class:\n",
    "        x2 = np.random.normal(size=1000).reshape(-1, 2) + 3.5\n",
    "        return np.concatenate([x0, x1, x2]), np.concatenate([np.zeros(500), np.ones(500), 2 + np.zeros(500)]).astype(int)\n",
    "    return np.concatenate([x0, x1]), np.concatenate([np.zeros(500), np.ones(500)]).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a. Create a dataset using the `create_data` function with `add_class` set to `False` and `add_outliers` set to `False`.\n",
    "   * Classify the output using Least Squares Classifier, Logistic Regression, Fisher's Linear Discriminant\n",
    "   * Plot the data points and the decision boundary for all the 3 models\n",
    "   * Write your observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Create a dataset using the `create_data` function with `add_class` set to `False` and `add_outliers` set to `True`.\n",
    "   * Classify the output using Least Squares Classifier, Logistic Regression, Fisher's Linear Discriminant\n",
    "   * Plot the data points and the decision boundary for all the 3 models\n",
    "   * Write your observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Multi-class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Create a dataset using the `create_data` function with `add_class` set to `True` and `add_outliers` set to `False`\n",
    "   * Classify the output using Least Squares Classifier and Logistic Regression\n",
    "   * Plot the data points and the decision boundary for above models\n",
    "   * Write your observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Use `abalone.csv`\n",
    "   * Consider any two columns as x_train\n",
    "   * Consider `class` column as y_train\n",
    "   * Classify the output using Least Squares Classifier and Logistic Regression\n",
    "   * Plot the data points and the decision boundary for above models\n",
    "   * Write your observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"abalone.csv\")\n",
    "sampled_df = df.sample(n=500)\n",
    "\n",
    "# x_train = sampled_df[['feature1','feature2']].values\n",
    "# y_train = sampled_df['target_variable'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from prml import nn\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Use `abalone.csv`\n",
    "* Consider `class` as the target variable and implement NN Classification without Regularization\n",
    "* Calculate accuracy metrics (accuracy, precision, recall, F1 score, confusion matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. Use `abalone.csv`\n",
    "* Consider `class` as the target variable and implement NN Classification with Regularization\n",
    "* Calculate accuracy metrics (accuracy, precision, recall, F1 score, confusion matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Convolution Neural Network Using `cnn_data.zip`\n",
    "* Download the `cnn_data.zip` from UBLearns and extract the contents. It contains `train` and `test` folders.\n",
    "* The images in this dataset are 240X240 pixels RGB (3 channels).\n",
    "* Implement CNN on this dataset \n",
    "* Calculate accuracy metrics (accuracy, precision, recall, F1 score, confusion matrix)\n",
    "* Write your observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from prml import nn\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Function to load the images from train and test folders\n",
    "def load_images_from_folder(folder_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for class_name in os.listdir(folder_path):\n",
    "        class_path = os.path.join(folder_path, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            for filename in os.listdir(class_path):\n",
    "                img = Image.open(os.path.join(class_path, filename))\n",
    "                img = img.resize((64, 64))\n",
    "                if img is not None:\n",
    "                    images.append(np.array(img))\n",
    "                    labels.append(class_name)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "train_dir = 'cnn_data/train'\n",
    "test_dir = 'cnn_data/test'\n",
    "train_images, train_labels = load_images_from_folder(train_dir)\n",
    "test_images, test_labels = load_images_from_folder(test_dir)\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "label_binarizer = LabelBinarizer()\n",
    "train_labels_one_hot = label_binarizer.fit_transform(train_labels)\n",
    "test_labels_one_hot = label_binarizer.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 711 images belonging to 3 classes.\n",
      "Found 114 images belonging to 3 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 3s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23/23 [==============================] - 85s 4s/step - loss: 4.4112 - accuracy: 0.3727 - val_loss: 0.8509 - val_accuracy: 0.6316\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 90s 4s/step - loss: 0.9431 - accuracy: 0.5302 - val_loss: 0.7956 - val_accuracy: 0.6228\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 86s 4s/step - loss: 0.9064 - accuracy: 0.5570 - val_loss: 0.7548 - val_accuracy: 0.7193\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.8319 - accuracy: 0.6118 - val_loss: 0.7036 - val_accuracy: 0.7281\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.7552 - accuracy: 0.6709 - val_loss: 0.6312 - val_accuracy: 0.7544\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.7932 - accuracy: 0.6526 - val_loss: 0.6292 - val_accuracy: 0.7456\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 86s 4s/step - loss: 0.7500 - accuracy: 0.6540 - val_loss: 0.6626 - val_accuracy: 0.7368\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 83s 4s/step - loss: 0.7457 - accuracy: 0.6526 - val_loss: 0.6250 - val_accuracy: 0.7368\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 111s 5s/step - loss: 0.7207 - accuracy: 0.6892 - val_loss: 0.5464 - val_accuracy: 0.8070\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 84s 4s/step - loss: 0.6760 - accuracy: 0.6835 - val_loss: 0.5721 - val_accuracy: 0.7982\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.5721 - accuracy: 0.7982\n",
      "Test Accuracy: 0.7982456088066101\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define data paths\n",
    "train_data_dir = \"cnn_data/train\"\n",
    "test_data_dir = \"cnn_data/test\"\n",
    "\n",
    "# Image dimensions\n",
    "img_height, img_width = 240, 240\n",
    "batch_size = 32\n",
    "\n",
    "# Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Create and compile a CNN model (VGG16 as an example)\n",
    "base_model = VGG16(include_top=False, weights='imagenet', input_shape=(img_height, img_width, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(3, activation='softmax'))  # 3 classes (adidas, converse, nike)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, epochs=10, validation_data=test_generator)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ConvolutionalNeuralNetwork at 0x2a0db01d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 10800000 into shape (50,60,60,20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m x_batch \u001b[38;5;241m=\u001b[39m x_train[indices[index: index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m50\u001b[39m]]\n\u001b[1;32m     40\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_train[indices[index: index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m50\u001b[39m]]\n\u001b[0;32m---> 41\u001b[0m logit \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m log_likelihood \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnn\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39msoftmax_cross_entropy(logit, y_batch)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39miter_count \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m, in \u001b[0;36mConvolutionalNeuralNetwork.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 20\u001b[0m     h \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb1)\n\u001b[1;32m     21\u001b[0m     h \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mmax_pooling2d(h, (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m))        \n\u001b[1;32m     22\u001b[0m     h \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(h) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb2)\n",
      "File \u001b[0;32m~/Documents/UB Fall 2023/CSE 574 ML/Homeworks/HW2/prml/nn/image/convolve2d.py:64\u001b[0m, in \u001b[0;36mConvolve2d.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     63\u001b[0m     func \u001b[38;5;241m=\u001b[39m Convolve2dFunction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad)\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/UB Fall 2023/CSE 574 ML/Homeworks/HW2/prml/nn/function.py:15\u001b[0m, in \u001b[0;36mFunction.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autobroadcast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m---> 15\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39menable_backprop:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Array(out, parent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/UB Fall 2023/CSE 574 ML/Homeworks/HW2/prml/nn/image/convolve2d.py:32\u001b[0m, in \u001b[0;36mConvolve2dFunction._forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutshape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m+\u001b[39m (y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_flattened \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_flattened\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 10800000 into shape (50,60,60,20)"
     ]
    }
   ],
   "source": [
    "class ConvolutionalNeuralNetwork(nn.Network):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        with self.set_parameter():\n",
    "            self.conv1 = nn.image.Convolve2d(\n",
    "                nn.random.truncnormal(-2, 2, 1, (5, 5, 1, 20)),\n",
    "                stride=(1, 1), pad=(0, 0))\n",
    "            self.b1 = nn.array([0.1] * 20)\n",
    "            self.conv2 = nn.image.Convolve2d(\n",
    "                nn.random.truncnormal(-2, 2, 1, (5, 5, 20, 20)),\n",
    "                stride=(1, 1), pad=(0, 0))\n",
    "            self.b2 = nn.array([0.1] * 20)\n",
    "            self.w3 = nn.random.truncnormal(-2, 2, 1, (4 * 4 * 20, 100))\n",
    "            self.b3 = nn.array([0.1] * 100)\n",
    "            self.w4 = nn.random.truncnormal(-2, 2, 1, (100, 10))\n",
    "            self.b4 = nn.array([0.1] * 10)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        h = nn.relu(self.conv1(x) + self.b1)\n",
    "        h = nn.max_pooling2d(h, (2, 2), (2, 2))        \n",
    "        h = nn.relu(self.conv2(h) + self.b2)\n",
    "        h = nn.max_pooling2d(h, (2, 2), (2, 2))\n",
    "        h = h.reshape(-1, 4 * 4 * 20)\n",
    "        h = nn.relu(h @ self.w3 + self.b3)\n",
    "        return h @ self.w4 + self.b4\n",
    "\n",
    "model = ConvolutionalNeuralNetwork()\n",
    "optimizer = nn.optimizer.Adam(model.parameter, 1e-3)\n",
    "x_train = train_images\n",
    "y_train = train_labels_one_hot\n",
    "x_test = test_images\n",
    "label_test = test_labels_one_hot\n",
    "\n",
    "while True:\n",
    "    indices = np.random.permutation(len(x_train))\n",
    "    for index in range(0, len(x_train), 50):\n",
    "        model.clear()\n",
    "        x_batch = x_train[indices[index: index + 50]]\n",
    "        y_batch = y_train[indices[index: index + 50]]\n",
    "        logit = model(x_batch)\n",
    "        log_likelihood = -nn.loss.softmax_cross_entropy(logit, y_batch).mean(0).sum()\n",
    "        if optimizer.iter_count % 100 == 0:\n",
    "            accuracy = accuracy_score(\n",
    "                np.argmax(y_batch, axis=-1), np.argmax(logit.value, axis=-1)\n",
    "            )\n",
    "            print(\"step {:04d}\".format(optimizer.iter_count), end=\", \")\n",
    "            print(\"accuracy {:.2f}\".format(accuracy), end=\", \")\n",
    "            print(\"Log Likelihood {:g}\".format(log_likelihood.value[0]))\n",
    "        optimizer.maximize(log_likelihood)\n",
    "        if optimizer.iter_count == 1000:\n",
    "            break\n",
    "    else:\n",
    "        continue\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy, precision, recall, F1 score and Confusion Matrix\n",
    "predictions = np.argmax(model(x_test).value, axis=-1)\n",
    "accuracy = accuracy_score(np.argmax(model(x_test).value, axis=-1), label_test)\n",
    "precision = precision_score(label_test, predictions, average='weighted')\n",
    "recall = recall_score(label_test, predictions, average='weighted')\n",
    "f1 = f1_score(label_test, predictions, average='weighted')\n",
    "confusion = confusion_matrix(label_test, predictions)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\\n\", confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: (Bonus) Multi-class Classification using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5a. Use `bonus.csv`\n",
    "   * Consider `quality` column as target variable\n",
    "   * Split the dataset into train and test sets (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "   * Train the model using Logistic Regression (with and without Regularization)\n",
    "   (https://scikit-learn.org/stable/modules/linear_model.html)\n",
    "   * Test the model on test set\n",
    "   * Calculate accuracy, precision, recall, F-1 score, confusion matrix for both models (https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)\n",
    "   * Write your observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
